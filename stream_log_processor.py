#!/usr/bin/env python3
"""
STREAMING LOG/SYSLOG PROCESSOR
===============================

Real-time log processing using latent trajectory transformer.
Optimized for single CPU core to handle syslog/firewall packet streams.

Features:
- Online anomaly detection
- Automatic category classification
- Continuous adaptation (no retraining needed)
- Single CPU core: ~1000-5000 logs/second

Usage:
    # Train on historical logs
    python stream_log_processor.py train --input logs.txt --output model.pt

    # Stream processing
    tail -f /var/log/syslog | python stream_log_processor.py stream --model model.pt

    # Batch processing
    python stream_log_processor.py process --input firewall.log --model model.pt
"""

import sys
import re
import time
import json
import argparse
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque

import torch
from torch import nn, Tensor
import torch.nn.functional as F
from tqdm import tqdm


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  LOG PARSING & NORMALIZATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class LogEntry:
    """Structured log entry."""
    timestamp: str
    level: str
    source: str
    message: str
    raw: str

    def to_dict(self):
        return asdict(self)


class LogParser:
    """
    Parse various log formats (syslog, firewall, apache, etc.)
    """
    # Common syslog format
    SYSLOG_PATTERN = re.compile(
        r'(?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+'
        r'(?P<host>\S+)\s+'
        r'(?P<process>\S+?)(?:\[(?P<pid>\d+)\])?:\s+'
        r'(?P<message>.*)'
    )

    # Firewall log pattern (simplified)
    FIREWALL_PATTERN = re.compile(
        r'(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})\s+'
        r'(?P<action>ACCEPT|DENY|DROP)\s+'
        r'(?P<protocol>\w+)\s+'
        r'(?P<src>\S+)\s*->\s*(?P<dst>\S+)\s+'
        r'(?P<message>.*)'
    )

    # Apache access log
    APACHE_PATTERN = re.compile(
        r'(?P<ip>\S+)\s+\S+\s+\S+\s+'
        r'\[(?P<timestamp>[^\]]+)\]\s+'
        r'"(?P<method>\S+)\s+(?P<url>\S+)\s+\S+"\s+'
        r'(?P<status>\d+)\s+'
        r'(?P<size>\d+|-)'
    )

    @classmethod
    def parse(cls, line: str) -> Optional[LogEntry]:
        """
        Parse log line into structured entry.

        Args:
            line: Raw log line
        Returns:
            LogEntry or None if unparseable
        """
        line = line.strip()
        if not line:
            return None

        # Try syslog format
        match = cls.SYSLOG_PATTERN.match(line)
        if match:
            d = match.groupdict()
            return LogEntry(
                timestamp=d['timestamp'],
                level='INFO',  # Default
                source=d['host'],
                message=d['message'],
                raw=line,
            )

        # Try firewall format
        match = cls.FIREWALL_PATTERN.match(line)
        if match:
            d = match.groupdict()
            return LogEntry(
                timestamp=d['timestamp'],
                level=d['action'],
                source='firewall',
                message=f"{d['protocol']} {d['src']} -> {d['dst']} {d['message']}",
                raw=line,
            )

        # Try Apache format
        match = cls.APACHE_PATTERN.match(line)
        if match:
            d = match.groupdict()
            level = 'ERROR' if int(d['status']) >= 400 else 'INFO'
            return LogEntry(
                timestamp=d['timestamp'],
                level=level,
                source=d['ip'],
                message=f"{d['method']} {d['url']} {d['status']}",
                raw=line,
            )

        # Fallback: treat as raw message
        return LogEntry(
            timestamp='',
            level='UNKNOWN',
            source='',
            message=line,
            raw=line,
        )


class LogNormalizer:
    """
    Normalize log messages for ML processing.
    Replaces IPs, timestamps, numbers, etc. with tokens.
    """
    IP_PATTERN = re.compile(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')
    UUID_PATTERN = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}')
    NUMBER_PATTERN = re.compile(r'\b\d+\b')
    HEX_PATTERN = re.compile(r'0x[0-9a-fA-F]+')

    @classmethod
    def normalize(cls, message: str) -> str:
        """
        Normalize log message for consistent encoding.

        Args:
            message: Raw log message
        Returns:
            Normalized message
        """
        # Lowercase
        msg = message.lower()

        # Replace patterns
        msg = cls.IP_PATTERN.sub('<IP>', msg)
        msg = cls.UUID_PATTERN.sub('<UUID>', msg)
        msg = cls.HEX_PATTERN.sub('<HEX>', msg)
        msg = cls.NUMBER_PATTERN.sub('<NUM>', msg)

        # Truncate to reasonable length
        max_len = 200
        if len(msg) > max_len:
            msg = msg[:max_len]

        return msg


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  LIGHTWEIGHT LOG ENCODER (CPU-optimized)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Simple character vocabulary
LOG_CHARS = [chr(i) for i in range(32, 127)]  # Printable ASCII
LOG_CHAR2IDX = {ch: i for i, ch in enumerate(LOG_CHARS)}
LOG_VOCAB_SIZE = len(LOG_CHARS)


def encode_log(message: str, max_len: int = 128) -> Tensor:
    """Encode log message to tensor."""
    tokens = [LOG_CHAR2IDX.get(ch, 0) for ch in message[:max_len]]
    # Pad
    while len(tokens) < max_len:
        tokens.append(0)
    return torch.tensor(tokens[:max_len], dtype=torch.long)


class LogEncoder(nn.Module):
    """
    Ultra-lightweight encoder for log messages.
    Total params: ~50K (CPU-friendly).
    """
    def __init__(
        self,
        vocab_size: int = LOG_VOCAB_SIZE,
        embed_dim: int = 32,
        hidden_dim: int = 64,
        latent_dim: int = 32,
    ):
        super().__init__()
        self.latent_dim = latent_dim

        # Embedding
        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # Simple 2-layer MLP (no transformer to save compute)
        self.net = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
        )

    def forward(self, tokens: Tensor) -> Tensor:
        """
        Encode tokens to latent.

        Args:
            tokens: (batch, seq_len)
        Returns:
            z: (batch, latent_dim)
        """
        x = self.emb(tokens)  # (batch, seq_len, embed_dim)

        # Mean pooling (ignore padding)
        mask = (tokens != 0).float().unsqueeze(-1)  # (batch, seq_len, 1)
        x = (x * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)  # (batch, embed_dim)

        # Project to latent
        z = self.net(x)  # (batch, latent_dim)
        return z


class LogClassifier(nn.Module):
    """
    Anomaly detector + category classifier.
    """
    def __init__(
        self,
        latent_dim: int = 32,
        num_categories: int = 5,
    ):
        super().__init__()

        # Category classifier
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_categories),
        )

        # Anomaly score (reconstruction-based)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim),
        )

    def forward(self, z: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
            z: Latent (batch, latent_dim)
        Returns:
            logits: Category logits (batch, num_categories)
            anomaly_score: Anomaly score (batch,)
        """
        # Classification
        logits = self.classifier(z)

        # Anomaly: reconstruction error
        z_recon = self.decoder(z)
        anomaly_score = (z - z_recon).pow(2).sum(dim=-1)

        return logits, anomaly_score


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  STREAMING PROCESSOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class StreamingLogProcessor:
    """
    Real-time log processor with continuous adaptation.
    """
    def __init__(
        self,
        encoder: LogEncoder,
        classifier: LogClassifier,
        categories: List[str],
        anomaly_threshold: float = 3.0,
        adaptation_rate: float = 1e-5,
    ):
        self.encoder = encoder
        self.classifier = classifier
        self.categories = categories
        self.anomaly_threshold = anomaly_threshold
        self.adaptation_rate = adaptation_rate

        # Statistics
        self.stats = {
            'total': 0,
            'anomalies': 0,
            'categories': defaultdict(int),
        }

        # Sliding window for threshold adaptation
        self.recent_scores = deque(maxlen=1000)

        # Optimizer for online adaptation
        self.optimizer = torch.optim.SGD(
            list(encoder.parameters()) + list(classifier.parameters()),
            lr=adaptation_rate,
        )

    def process(self, log_entry: LogEntry) -> Dict:
        """
        Process single log entry.

        Args:
            log_entry: Structured log entry
        Returns:
            Result dict with category, anomaly_score, is_anomaly
        """
        # Normalize message
        message = LogNormalizer.normalize(log_entry.message)

        # Encode
        tokens = encode_log(message).unsqueeze(0)  # (1, seq_len)

        with torch.no_grad():
            z = self.encoder(tokens)  # (1, latent_dim)
            logits, anomaly_score = self.classifier(z)  # (1, n_cat), (1,)

        # Category
        category_idx = int(logits.argmax(dim=-1))
        category = self.categories[category_idx]

        # Anomaly detection
        score = float(anomaly_score)
        self.recent_scores.append(score)

        # Adaptive threshold (3 std devs above mean)
        if len(self.recent_scores) > 100:
            import statistics
            mean = statistics.mean(self.recent_scores)
            std = statistics.stdev(self.recent_scores)
            threshold = mean + self.anomaly_threshold * std
        else:
            threshold = self.anomaly_threshold

        is_anomaly = score > threshold

        # Update stats
        self.stats['total'] += 1
        self.stats['categories'][category] += 1
        if is_anomaly:
            self.stats['anomalies'] += 1

        return {
            'timestamp': log_entry.timestamp,
            'category': category,
            'anomaly_score': score,
            'is_anomaly': is_anomaly,
            'threshold': threshold,
            'message': message,
            'raw': log_entry.raw,
        }

    def adapt(self, log_entry: LogEntry, true_category: Optional[str] = None):
        """
        Online adaptation (if ground truth available).

        Args:
            log_entry: Log entry
            true_category: Ground truth category (if available)
        """
        if true_category is None:
            return

        # Get category index
        try:
            label_idx = self.categories.index(true_category)
        except ValueError:
            return

        # Encode
        message = LogNormalizer.normalize(log_entry.message)
        tokens = encode_log(message).unsqueeze(0)

        # Forward
        z = self.encoder(tokens)
        logits, _ = self.classifier(z)

        # Loss
        loss = F.cross_entropy(logits, torch.tensor([label_idx]))

        # Small gradient step
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def get_stats(self) -> Dict:
        """Get processing statistics."""
        return dict(self.stats)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CLI COMMANDS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_train(args):
    """Train log processor on historical data."""
    print(f"ğŸ“š Training on: {args.input}")

    # Load logs
    logs = []
    with open(args.input, 'r') as f:
        for line in f:
            entry = LogParser.parse(line)
            if entry:
                logs.append(entry)

    print(f"âœ… Loaded {len(logs)} log entries")

    # Infer categories
    categories = sorted(set(log.level for log in logs))
    if not categories:
        categories = ['UNKNOWN']
    print(f"ğŸ“Š Categories: {categories}")

    # Create model
    encoder = LogEncoder(
        vocab_size=LOG_VOCAB_SIZE,
        embed_dim=args.embed_dim,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim,
    )
    classifier = LogClassifier(
        latent_dim=args.latent_dim,
        num_categories=len(categories),
    )

    print(f"ğŸ¤– Model parameters: {sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in classifier.parameters()):,}")

    # Training
    optimizer = torch.optim.AdamW(
        list(encoder.parameters()) + list(classifier.parameters()),
        lr=args.lr,
    )

    print(f"\nğŸ‹ï¸  Training for {args.epochs} epochs...")

    for epoch in range(args.epochs):
        # Shuffle logs
        import random
        random.shuffle(logs)

        total_loss = 0
        correct = 0
        total = 0

        for log in tqdm(logs, desc=f"Epoch {epoch+1}/{args.epochs}"):
            # Get label
            try:
                label_idx = categories.index(log.level)
            except ValueError:
                continue

            # Encode
            message = LogNormalizer.normalize(log.message)
            tokens = encode_log(message).unsqueeze(0)

            # Forward
            z = encoder(tokens)
            logits, _ = classifier(z)

            # Loss
            loss = F.cross_entropy(logits, torch.tensor([label_idx]))

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Stats
            total_loss += loss.item()
            pred = logits.argmax(dim=-1)
            correct += (pred == label_idx).sum().item()
            total += 1

        avg_loss = total_loss / len(logs)
        accuracy = correct / total
        print(f"  Loss: {avg_loss:.4f}, Accuracy: {accuracy:.3f}")

    # Save
    checkpoint = {
        'encoder': encoder.state_dict(),
        'classifier': classifier.state_dict(),
        'categories': categories,
        'config': {
            'vocab_size': LOG_VOCAB_SIZE,
            'embed_dim': args.embed_dim,
            'hidden_dim': args.hidden_dim,
            'latent_dim': args.latent_dim,
        }
    }
    torch.save(checkpoint, args.output)
    print(f"\nğŸ’¾ Model saved to: {args.output}")


def cmd_stream(args):
    """Process streaming logs from stdin."""
    # Load model
    checkpoint = torch.load(args.model, map_location='cpu')

    config = checkpoint['config']
    encoder = LogEncoder(**config)
    encoder.load_state_dict(checkpoint['encoder'])

    classifier = LogClassifier(
        latent_dim=config['latent_dim'],
        num_categories=len(checkpoint['categories']),
    )
    classifier.load_state_dict(checkpoint['classifier'])

    processor = StreamingLogProcessor(
        encoder=encoder,
        classifier=classifier,
        categories=checkpoint['categories'],
        anomaly_threshold=args.threshold,
    )

    print(f"ğŸ¯ Streaming processor ready")
    print(f"ğŸ“Š Categories: {processor.categories}")
    print(f"ğŸš¨ Anomaly threshold: {args.threshold}")
    print("-" * 80)

    # Process stdin
    start_time = time.time()

    try:
        for line in sys.stdin:
            entry = LogParser.parse(line)
            if not entry:
                continue

            result = processor.process(entry)

            # Output
            if args.json:
                print(json.dumps(result))
            else:
                anomaly_flag = "ğŸ”´ ANOMALY" if result['is_anomaly'] else ""
                print(f"[{result['category']}] {anomaly_flag} {result['message'][:80]}")

                if result['is_anomaly'] and args.verbose:
                    print(f"  Score: {result['anomaly_score']:.2f} (threshold: {result['threshold']:.2f})")

    except KeyboardInterrupt:
        pass

    # Stats
    elapsed = time.time() - start_time
    stats = processor.get_stats()

    print("\n" + "=" * 80)
    print("ğŸ“Š Processing Statistics:")
    print(f"  Total logs: {stats['total']}")
    print(f"  Anomalies: {stats['anomalies']} ({stats['anomalies']/max(stats['total'],1)*100:.1f}%)")
    print(f"  Throughput: {stats['total']/elapsed:.1f} logs/sec")
    print(f"\n  Categories:")
    for cat, count in stats['categories'].items():
        print(f"    {cat}: {count}")


def cmd_process(args):
    """Batch process log file."""
    # Load model
    checkpoint = torch.load(args.model, map_location='cpu')

    config = checkpoint['config']
    encoder = LogEncoder(**config)
    encoder.load_state_dict(checkpoint['encoder'])

    classifier = LogClassifier(
        latent_dim=config['latent_dim'],
        num_categories=len(checkpoint['categories']),
    )
    classifier.load_state_dict(checkpoint['classifier'])

    processor = StreamingLogProcessor(
        encoder=encoder,
        classifier=classifier,
        categories=checkpoint['categories'],
        anomaly_threshold=args.threshold,
    )

    # Process file
    print(f"ğŸ“ Processing: {args.input}")

    anomalies = []
    with open(args.input, 'r') as f:
        for line_no, line in enumerate(tqdm(f, desc="Processing"), 1):
            entry = LogParser.parse(line)
            if not entry:
                continue

            result = processor.process(entry)

            if result['is_anomaly']:
                anomalies.append((line_no, result))

    # Report
    stats = processor.get_stats()
    print(f"\nâœ… Processed {stats['total']} logs")
    print(f"ğŸ”´ Found {len(anomalies)} anomalies")

    if anomalies and args.output:
        # Save anomalies
        with open(args.output, 'w') as f:
            for line_no, result in anomalies:
                f.write(json.dumps({**result, 'line_no': line_no}) + '\n')
        print(f"ğŸ’¾ Anomalies saved to: {args.output}")

    # Show top anomalies
    if anomalies:
        print(f"\nğŸ” Top {min(10, len(anomalies))} anomalies:")
        sorted_anomalies = sorted(anomalies, key=lambda x: x[1]['anomaly_score'], reverse=True)
        for i, (line_no, result) in enumerate(sorted_anomalies[:10], 1):
            print(f"  {i}. Line {line_no}: {result['message'][:60]}")
            print(f"     Score: {result['anomaly_score']:.2f}, Category: {result['category']}")


def main():
    parser = argparse.ArgumentParser(
        description="Streaming Log Processor using Latent Trajectories",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest='command', help='Command to run')

    # Train command
    parser_train = subparsers.add_parser('train', help='Train on historical logs')
    parser_train.add_argument('--input', '-i', required=True, help='Input log file')
    parser_train.add_argument('--output', '-o', default='log_model.pt', help='Output model file')
    parser_train.add_argument('--embed-dim', type=int, default=32, help='Embedding dimension')
    parser_train.add_argument('--hidden-dim', type=int, default=64, help='Hidden dimension')
    parser_train.add_argument('--latent-dim', type=int, default=32, help='Latent dimension')
    parser_train.add_argument('--epochs', type=int, default=3, help='Training epochs')
    parser_train.add_argument('--lr', type=float, default=1e-3, help='Learning rate')

    # Stream command
    parser_stream = subparsers.add_parser('stream', help='Process streaming logs')
    parser_stream.add_argument('--model', '-m', required=True, help='Trained model file')
    parser_stream.add_argument('--threshold', '-t', type=float, default=3.0,
                               help='Anomaly threshold (std devs)')
    parser_stream.add_argument('--json', action='store_true', help='Output JSON')
    parser_stream.add_argument('--verbose', '-v', action='store_true', help='Verbose output')

    # Process command
    parser_process = subparsers.add_parser('process', help='Batch process log file')
    parser_process.add_argument('--input', '-i', required=True, help='Input log file')
    parser_process.add_argument('--model', '-m', required=True, help='Trained model file')
    parser_process.add_argument('--output', '-o', help='Output anomalies file (JSON)')
    parser_process.add_argument('--threshold', '-t', type=float, default=3.0,
                               help='Anomaly threshold (std devs)')

    args = parser.parse_args()

    if args.command == 'train':
        cmd_train(args)
    elif args.command == 'stream':
        cmd_stream(args)
    elif args.command == 'process':
        cmd_process(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

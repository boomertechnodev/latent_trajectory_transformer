"""
Transformer Integration Guide and Testing Framework
Shows how to integrate improved components into latent_drift_trajectory.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
import matplotlib.pyplot as plt
import numpy as np

# Import improved components
from transformer_improvements import (
    RotaryPositionEmbedding,
    ALiBiPositionalBias,
    FlashMultiHeadAttention,
    OptimizedTransformerBlock,
    ImprovedEncoder,
    ImprovedDecoder,
    SwiGLU,
    visualize_attention_patterns,
    analyze_gradient_flow,
)


# ============================================================================
# DROP-IN REPLACEMENTS FOR EXISTING COMPONENTS
# ============================================================================

class ModernQKVAttention(nn.Module):
    """
    Drop-in replacement for QKVAttention (lines 584-618).

    Key improvements:
    - RoPE instead of sinusoidal positioning
    - Flash Attention optimization
    - Better initialization with MAGNETO scaling
    - Optional ALiBi bias
    """

    def __init__(
        self,
        dim_in: int,
        dim_qk: int,
        dim_v: int,
        nb_heads: int = 1,
        causal: bool = False,
        dropout: float = 0.0
    ):
        super().__init__()

        # Create improved attention module
        self.attention = FlashMultiHeadAttention(
            d_model=dim_in,
            n_heads=nb_heads,
            dropout=dropout,
            causal=causal,
            use_rope=True,  # Use RoPE by default
            use_alibi=False,  # Can enable for length extrapolation
            use_flash=True,  # Enable Flash Attention if available
        )

        # Match original interface
        self.causal = causal
        self.dropout = dropout

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass matching original QKVAttention interface.

        Args:
            x: (N, T, C) - batch, time, channels

        Returns:
            y: (N, T, C) - attention output
        """
        output, _ = self.attention(x, mask=None, use_cache=False)
        return output


class ModernTransformerBlock(nn.Module):
    """
    Drop-in replacement for TransformerBlock (lines 621-651).

    Key improvements:
    - Pre-norm instead of post-norm for stability
    - SwiGLU instead of ReLU for better expressiveness
    - Gradient checkpointing support
    - Better residual initialization
    """

    def __init__(
        self,
        dim_model: int,
        dim_keys: int,
        dim_hidden: int,
        nb_heads: int,
        causal: bool,
        dropout: float
    ):
        super().__init__()

        # Use modern optimized block
        self.block = OptimizedTransformerBlock(
            d_model=dim_model,
            n_heads=nb_heads,
            d_ff=dim_hidden,
            dropout=dropout,
            activation="swiglu",  # Better than ReLU
            norm_type="pre",  # More stable than post-norm
            causal=causal,
            use_rope=True,
            use_alibi=False,
            checkpoint=False,  # Can enable for memory savings
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Match original interface."""
        output, _ = self.block(x, mask=None)
        return output


class ModernAddPositionalEncoding(nn.Module):
    """
    Drop-in replacement for AddPositionalEncoding (lines 570-581).

    Uses RoPE internally but maintains the same interface.
    """

    def __init__(self, len_max: float):
        super().__init__()
        self.len_max = int(len_max)
        # We'll apply positioning in attention instead
        self.dummy = nn.Parameter(torch.zeros(1))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        In modern architecture, position encoding is handled by attention.
        This is kept for interface compatibility.
        """
        return x  # Position will be added in attention layer


# ============================================================================
# UPGRADED MODELS WITH MINIMAL CHANGES
# ============================================================================

class UpgradedPosteriorEncoder(nn.Module):
    """
    Upgraded version of PosteriorEncoder (lines 463-504).

    Minimal changes for easy integration:
    - Replace TransformerBlock with OptimizedTransformerBlock
    - Use RoPE instead of sinusoidal positioning
    - Better initialization
    """

    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int):
        super().__init__()

        self.emb = nn.Embedding(vocab_size, embed_size)
        nn.init.normal_(self.emb.weight, mean=0.0, std=0.02)

        # Project if dimensions don't match
        if embed_size != hidden_size:
            self.in_proj = nn.Linear(embed_size, hidden_size)
        else:
            self.in_proj = nn.Identity()

        # Configuration
        nb_heads = 4
        nb_blocks = 4
        dropout = 0.0
        causal = False

        # Use modern transformer blocks
        self.trunk = nn.ModuleList([
            OptimizedTransformerBlock(
                d_model=hidden_size,
                n_heads=nb_heads,
                d_ff=hidden_size * 4,  # Standard 4x expansion
                dropout=dropout,
                activation="swiglu",
                norm_type="pre",
                causal=causal,
                use_rope=True,
                use_alibi=False,
            )
            for _ in range(nb_blocks)
        ])

        # Final layer norm
        self.ln_f = nn.LayerNorm(hidden_size, eps=1e-6)

    def forward(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        Encode tokens to hidden states.

        Args:
            tokens: (B, L) token indices

        Returns:
            Hidden states (B, L, H)
        """
        # Embed and project
        x = self.emb(tokens)  # (B, L, E)
        x = self.in_proj(x)   # (B, L, H)

        # Apply transformer blocks (position encoding in attention)
        for block in self.trunk:
            x, _ = block(x)

        # Final normalization
        x = self.ln_f(x)

        return x


class UpgradedDiscreteObservation(nn.Module):
    """
    Upgraded version of DiscreteObservation (lines 389-456).

    Improvements:
    - Modern attention with RoPE
    - KV caching for efficient generation
    - Better initialization
    """

    def __init__(
        self,
        latent_size: int,
        vocab_size: int,
        embed_size: int,
        hidden_size: int,
        nb_heads: int = 4,
        dropout: float = 0.0,
    ):
        super().__init__()
        self.vocab_size = vocab_size

        # Embeddings
        self.token_emb = nn.Embedding(vocab_size, embed_size)
        nn.init.normal_(self.token_emb.weight, mean=0.0, std=0.02)

        # Projections
        self.latent_proj = nn.Linear(latent_size, hidden_size)
        self.token_proj = nn.Linear(embed_size, hidden_size)

        # Modern transformer block
        self.block = OptimizedTransformerBlock(
            d_model=hidden_size,
            n_heads=nb_heads,
            d_ff=hidden_size * 4,
            dropout=dropout,
            activation="swiglu",
            norm_type="pre",
            causal=True,  # Autoregressive
            use_rope=True,
            use_alibi=False,
        )

        # Output projection
        self.ln_f = nn.LayerNorm(hidden_size, eps=1e-6)
        self.proj_out = nn.Linear(hidden_size, vocab_size)

    def get_logits(
        self,
        z: torch.Tensor,
        tokens: torch.Tensor,
        past_kv: Optional[Tuple] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[Tuple]]:
        """
        Get logits with optional KV caching.

        Args:
            z: Latent states (B, L, D)
            tokens: Target tokens (B, L)
            past_kv: Cached KV pairs
            use_cache: Whether to return cache

        Returns:
            logits: (B, L, V)
            present_kv: Updated cache if use_cache=True
        """
        B, L, D = z.shape

        # Shift tokens right for teacher forcing
        tokens_in = tokens.roll(1, dims=1)
        tokens_in[:, 0] = 0  # Start token

        # Embed tokens
        tok_emb = self.token_emb(tokens_in)  # (B, L, E)

        # Combine with latent
        h = self.latent_proj(z) + self.token_proj(tok_emb)  # (B, L, H)

        # Apply transformer with caching
        h, present_kv = self.block(h, past_kv=past_kv, use_cache=use_cache)

        # Output projection
        h = self.ln_f(h)
        logits = self.proj_out(h)  # (B, L, V)

        return logits, present_kv

    def forward(self, z: torch.Tensor, tokens: torch.Tensor) -> torch.distributions.Distribution:
        """Original interface for compatibility."""
        logits, _ = self.get_logits(z, tokens, use_cache=False)
        return torch.distributions.Categorical(logits=logits.reshape(-1, self.vocab_size))


# ============================================================================
# TESTING AND VALIDATION FRAMEWORK
# ============================================================================

class TransformerTester:
    """
    Comprehensive testing suite for transformer improvements.
    """

    def __init__(self, device: torch.device = torch.device("cpu")):
        self.device = device

    def test_attention_patterns(self, old_model: nn.Module, new_model: nn.Module):
        """
        Compare attention patterns between old and new models.
        """
        batch_size = 2
        seq_len = 64
        vocab_size = 29
        hidden_size = 128

        # Create dummy input
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

        # Get outputs
        old_model.eval()
        new_model.eval()

        with torch.no_grad():
            old_out = old_model(input_ids)
            new_out = new_model(input_ids)

        # Compare outputs
        mse = F.mse_loss(old_out, new_out)
        cos_sim = F.cosine_similarity(
            old_out.flatten(),
            new_out.flatten(),
            dim=0
        )

        print(f"Output MSE: {mse:.6f}")
        print(f"Output Cosine Similarity: {cos_sim:.6f}")

        return {
            'mse': mse.item(),
            'cosine_similarity': cos_sim.item()
        }

    def test_gradient_flow(self, model: nn.Module):
        """
        Test gradient flow through the model.
        """
        batch_size = 2
        seq_len = 64
        vocab_size = 29

        # Create dummy data
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)
        target_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

        # Forward pass
        model.train()
        output = model(input_ids)

        # Compute loss
        if len(output.shape) == 3:
            output_flat = output.reshape(-1, output.size(-1))
            target_flat = target_ids.reshape(-1)
            loss = F.cross_entropy(output_flat, target_flat)
        else:
            loss = F.mse_loss(output, target_ids.float())

        # Analyze gradients
        grad_stats = analyze_gradient_flow(model, loss)

        # Print statistics
        print("\n=== Gradient Flow Analysis ===")
        if 'layer_grads' in grad_stats:
            for layer_name, stats in grad_stats['layer_grads'].items():
                print(f"{layer_name}: mean_norm={stats['mean_grad_norm']:.6f}")

        return grad_stats

    def test_memory_efficiency(self, old_model: nn.Module, new_model: nn.Module):
        """
        Compare memory usage between models.
        """
        import tracemalloc

        batch_size = 8
        seq_len = 128
        vocab_size = 29

        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

        # Test old model
        tracemalloc.start()
        old_model.eval()
        with torch.no_grad():
            _ = old_model(input_ids)
        old_memory = tracemalloc.get_traced_memory()[1] / 1024 / 1024  # MB
        tracemalloc.stop()

        # Test new model
        tracemalloc.start()
        new_model.eval()
        with torch.no_grad():
            _ = new_model(input_ids)
        new_memory = tracemalloc.get_traced_memory()[1] / 1024 / 1024  # MB
        tracemalloc.stop()

        print(f"\n=== Memory Usage ===")
        print(f"Old Model: {old_memory:.2f} MB")
        print(f"New Model: {new_memory:.2f} MB")
        print(f"Improvement: {(old_memory - new_memory) / old_memory * 100:.1f}%")

        return {
            'old_memory_mb': old_memory,
            'new_memory_mb': new_memory,
            'improvement_percent': (old_memory - new_memory) / old_memory * 100
        }

    def test_inference_speed(self, old_model: nn.Module, new_model: nn.Module, num_runs: int = 100):
        """
        Compare inference speed between models.
        """
        import time

        batch_size = 8
        seq_len = 128
        vocab_size = 29

        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=self.device)

        # Warmup
        for _ in range(10):
            with torch.no_grad():
                _ = old_model(input_ids)
                _ = new_model(input_ids)

        # Test old model
        old_model.eval()
        start = time.time()
        for _ in range(num_runs):
            with torch.no_grad():
                _ = old_model(input_ids)
        old_time = (time.time() - start) / num_runs

        # Test new model
        new_model.eval()
        start = time.time()
        for _ in range(num_runs):
            with torch.no_grad():
                _ = new_model(input_ids)
        new_time = (time.time() - start) / num_runs

        print(f"\n=== Inference Speed ===")
        print(f"Old Model: {old_time * 1000:.2f} ms/batch")
        print(f"New Model: {new_time * 1000:.2f} ms/batch")
        print(f"Speedup: {old_time / new_time:.2f}x")

        return {
            'old_time_ms': old_time * 1000,
            'new_time_ms': new_time * 1000,
            'speedup': old_time / new_time
        }

    def test_position_encoding(self):
        """
        Test and visualize different position encoding methods.
        """
        seq_len = 128
        d_model = 64

        # Create position encodings
        rope = RotaryPositionEmbedding(d_model, max_seq_len=seq_len)
        alibi = ALiBiPositionalBias(n_heads=4, max_seq_len=seq_len)

        # Test RoPE
        batch_size = 2
        n_heads = 4
        head_dim = d_model // n_heads

        q = torch.randn(batch_size, n_heads, seq_len, head_dim)
        k = torch.randn(batch_size, n_heads, seq_len, head_dim)

        q_rot, k_rot = rope.apply_rotary_pos_emb(q, k)

        # Verify shape preservation
        assert q_rot.shape == q.shape
        assert k_rot.shape == k.shape

        # Test ALiBi
        alibi_bias = alibi.get_bias(seq_len, q.device)
        assert alibi_bias.shape == (1, 4, seq_len, seq_len)

        print("\n=== Position Encoding Tests ===")
        print(f"âœ“ RoPE: shapes preserved")
        print(f"âœ“ ALiBi: correct bias shape")

        # Visualize ALiBi bias pattern
        plt.figure(figsize=(10, 4))
        for i in range(4):
            plt.subplot(1, 4, i+1)
            plt.imshow(alibi_bias[0, i, :32, :32].cpu().numpy(), cmap='coolwarm')
            plt.title(f'Head {i+1}')
            plt.colorbar()
        plt.suptitle('ALiBi Attention Bias Patterns (first 32 positions)')
        plt.tight_layout()
        plt.savefig('/home/user/latent_trajectory_transformer/alibi_patterns.png', dpi=150)
        print("âœ“ Saved ALiBi patterns to alibi_patterns.png")

        return True


# ============================================================================
# INTEGRATION EXAMPLES
# ============================================================================

def integrate_into_deterministic_ode_model():
    """
    Example: How to upgrade DeterministicLatentODE with modern components.
    """
    print("\n" + "="*70)
    print("INTEGRATION EXAMPLE: Upgrading DeterministicLatentODE")
    print("="*70)

    # Original imports would be from latent_drift_trajectory
    vocab_size = 29
    latent_size = 64
    hidden_size = 128
    embed_size = 64

    print("\n1. Replace encoder (lines 679-684):")
    print("   OLD: self.encoder = DeterministicEncoder(...)")
    print("   NEW: self.encoder = UpgradedDeterministicEncoder(...)")

    print("\n2. Replace decoder (lines 689-695):")
    print("   OLD: self.p_observe = DiscreteObservation(...)")
    print("   NEW: self.p_observe = UpgradedDiscreteObservation(...)")

    print("\n3. Optional: Replace individual components:")
    print("   - Replace QKVAttention with ModernQKVAttention")
    print("   - Replace TransformerBlock with ModernTransformerBlock")
    print("   - Replace AddPositionalEncoding with ModernAddPositionalEncoding")

    # Create comparison models
    from latent_drift_trajectory import (
        PosteriorEncoder as OldEncoder,
        DiscreteObservation as OldDecoder,
    )

    old_encoder = OldEncoder(vocab_size, embed_size, hidden_size)
    new_encoder = UpgradedPosteriorEncoder(vocab_size, embed_size, hidden_size)

    old_decoder = OldDecoder(latent_size, vocab_size, embed_size, hidden_size)
    new_decoder = UpgradedDiscreteObservation(latent_size, vocab_size, embed_size, hidden_size)

    # Test compatibility
    tester = TransformerTester()

    print("\n4. Testing encoder compatibility:")
    test_input = torch.randint(0, vocab_size, (2, 64))
    old_out = old_encoder(test_input)
    new_out = new_encoder(test_input)
    print(f"   Output shapes match: {old_out.shape == new_out.shape}")
    print(f"   Output similarity: {F.cosine_similarity(old_out.flatten(), new_out.flatten(), dim=0):.4f}")

    print("\n5. Testing decoder compatibility:")
    z = torch.randn(2, 64, latent_size)
    tokens = torch.randint(0, vocab_size, (2, 64))
    old_dist = old_decoder(z, tokens)
    new_dist = new_decoder(z, tokens)
    print(f"   Distribution types match: {type(old_dist) == type(new_dist)}")

    return True


def create_benchmarking_report():
    """
    Create comprehensive benchmarking report.
    """
    print("\n" + "="*70)
    print("TRANSFORMER OPTIMIZATION BENCHMARKING REPORT")
    print("="*70)

    device = torch.device("cpu")
    tester = TransformerTester(device)

    # Create test models
    vocab_size = 29
    hidden_size = 128
    embed_size = 64

    from latent_drift_trajectory import PosteriorEncoder as OldEncoder
    old_model = OldEncoder(vocab_size, embed_size, hidden_size)
    new_model = UpgradedPosteriorEncoder(vocab_size, embed_size, hidden_size)

    # Run tests
    print("\n### Testing Gradient Flow ###")
    grad_stats = tester.test_gradient_flow(new_model)

    print("\n### Testing Memory Efficiency ###")
    memory_stats = tester.test_memory_efficiency(old_model, new_model)

    print("\n### Testing Inference Speed ###")
    speed_stats = tester.test_inference_speed(old_model, new_model)

    print("\n### Testing Position Encodings ###")
    tester.test_position_encoding()

    # Summary
    print("\n" + "="*70)
    print("SUMMARY OF IMPROVEMENTS")
    print("="*70)
    print("\nâœ… Architectural Improvements:")
    print("   - RoPE/ALiBi for better position encoding")
    print("   - Flash Attention for O(n) memory complexity")
    print("   - Pre-norm for stable deep models")
    print("   - SwiGLU for better expressiveness")
    print("   - KV caching for efficient generation")

    print("\nâœ… Performance Gains:")
    if 'improvement_percent' in memory_stats:
        print(f"   - Memory reduction: {memory_stats['improvement_percent']:.1f}%")
    if 'speedup' in speed_stats:
        print(f"   - Inference speedup: {speed_stats['speedup']:.2f}x")

    print("\nâœ… Integration Path:")
    print("   1. Start with drop-in replacements")
    print("   2. Test compatibility thoroughly")
    print("   3. Gradually adopt advanced features")
    print("   4. Profile and optimize based on needs")


if __name__ == "__main__":
    # Run integration examples
    integrate_into_deterministic_ode_model()

    # Create benchmarking report
    create_benchmarking_report()

    print("\n" + "="*70)
    print("ðŸš€ TRANSFORMER UPGRADE COMPLETE!")
    print("="*70)
    print("\nNext steps:")
    print("1. Review transformer_improvements.py for detailed implementations")
    print("2. Use drop-in replacements for easy integration")
    print("3. Run tests to validate improvements")
    print("4. Gradually adopt advanced features like Flash Attention")
    print("\nðŸ”® Your transformers are now state-of-the-art!")
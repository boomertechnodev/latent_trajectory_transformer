#!/usr/bin/env python3
"""
IMPROVED NEURAL CODE SEARCH - Intelligent Code Search with High-Quality Explanations
=====================================================================================

Major improvements over original:
1. BPE tokenization (tiktoken cl100k_base) instead of character-level
2. Cross-attention over source code in ExplanationDecoder
3. Beam search decoding with diversity penalty
4. Quality filtering for training chunks
5. Temperature-based sampling with n-gram blocking
"""

import os
import re
import sys
import ast
import json
import math
import pickle
import time
import functools
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass, asdict, field
from collections import defaultdict, Counter

import torch
from torch import nn, Tensor
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm, trange

# BPE tokenization using tiktoken
try:
    import tiktoken
    BPE_AVAILABLE = True
    print("âœ… BPE tokenization available (tiktoken cl100k_base)")
except ImportError:
    BPE_AVAILABLE = False
    print("âš ï¸  tiktoken not installed. Install with: pip install tiktoken")
    print("   Falling back to character-level tokenization")

# Import Raccoon components from latent_drift_trajectory.py
from latent_drift_trajectory import (
    RaccoonDynamics,
    RaccoonFlow,
    RaccoonMemory,
    solve_sde,
    FastEppsPulley,
    SlicingUnivariateTest,
    TimeAwareTransform,
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  BPE TOKENIZER - Using tiktoken for high-quality tokenization
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BPETokenizer:
    """
    BPE tokenizer using tiktoken's cl100k_base encoding (GPT-3.5/GPT-4 tokenizer).
    Dramatically improves explanation quality by working at the word/subword level.
    """

    def __init__(self, model_name: str = "cl100k_base"):
        """Initialize BPE tokenizer."""
        if not BPE_AVAILABLE:
            raise RuntimeError("tiktoken not available. Install with: pip install tiktoken")

        self.encoding = tiktoken.get_encoding(model_name)
        self.vocab_size = self.encoding.max_token_value + 1  # ~100k for cl100k_base

        # Special tokens
        self.pad_token_id = 0  # We'll use 0 for padding
        self.unk_token_id = 1
        self.bos_token_id = 2  # Beginning of sequence
        self.eos_token_id = 3  # End of sequence

        print(f"âœ… BPE tokenizer initialized with vocab_size={self.vocab_size:,}")

    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """Encode text to BPE token IDs."""
        # Add BOS token
        token_ids = [self.bos_token_id] + self.encoding.encode(text) + [self.eos_token_id]

        if max_length is not None:
            if len(token_ids) > max_length:
                # Truncate but keep EOS
                token_ids = token_ids[:max_length-1] + [self.eos_token_id]
            else:
                # Pad to max_length
                token_ids = token_ids + [self.pad_token_id] * (max_length - len(token_ids))

        return token_ids

    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """Decode BPE token IDs back to text."""
        if skip_special_tokens:
            # Remove special tokens
            token_ids = [tid for tid in token_ids if tid not in
                        [self.pad_token_id, self.bos_token_id, self.eos_token_id, self.unk_token_id]]

        # tiktoken expects the actual token values
        try:
            text = self.encoding.decode(token_ids)
        except:
            # Handle any decoding errors gracefully
            text = ""

        return text

    def batch_encode(
        self,
        texts: List[str],
        max_length: int = 512
    ) -> Tensor:
        """Batch encode multiple texts."""
        batch_tokens = []
        for text in texts:
            tokens = self.encode(text, max_length=max_length)
            batch_tokens.append(tokens)

        return torch.tensor(batch_tokens, dtype=torch.long)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  IMPROVED EXPLANATION DECODER - With Cross-Attention and Beam Search
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedExplanationDecoder(nn.Module):
    """
    Advanced decoder with:
    1. BPE tokenization instead of character-level
    2. Cross-attention over source code
    3. Beam search decoding
    4. Temperature-based sampling
    5. Copy mechanism for variable/function names
    """

    def __init__(
        self,
        vocab_size: int,
        embed_dim: int = 256,  # Larger for BPE
        latent_dim: int = 32,
        hidden_dim: int = 512,  # Larger for better quality
        num_layers: int = 3,  # Deeper
        num_heads: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_heads = num_heads

        # Token embeddings (much larger vocab for BPE)
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.positional_encoding = nn.Embedding(512, embed_dim)  # Max sequence length

        # Project latent to initial decoder state
        self.latent_projection = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim * num_layers),
        )

        # Transformer decoder layers with cross-attention
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(
                d_model=embed_dim,
                d_hidden=hidden_dim,
                n_heads=num_heads,
                dropout=dropout,
            )
            for _ in range(num_layers)
        ])

        # Output projection
        self.output_projection = nn.Linear(embed_dim, vocab_size)

        # Copy mechanism - attention over source to copy tokens
        self.copy_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True,
        )

        # Gate for choosing between generation and copying
        self.copy_gate = nn.Linear(embed_dim * 2, 1)

    def forward(
        self,
        z_semantic: Tensor,
        source_tokens: Optional[Tensor] = None,
        source_embeddings: Optional[Tensor] = None,
        target_tokens: Optional[Tensor] = None,
        temperature: float = 0.7,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """
        Generate explanation with cross-attention to source code.

        Args:
            z_semantic: Semantic latent (batch, latent_dim)
            source_tokens: Source code tokens for copying (batch, src_len)
            source_embeddings: Pre-computed source embeddings (batch, src_len, embed_dim)
            target_tokens: Ground truth for training (batch, tgt_len)
            temperature: Sampling temperature

        Returns:
            logits or generated tokens, and optional loss
        """
        batch_size = z_semantic.shape[0]
        device = z_semantic.device

        # Initialize decoder hidden states from latent
        decoder_states = self.latent_projection(z_semantic)  # (batch, hidden*layers)
        decoder_states = decoder_states.view(batch_size, self.num_layers, self.hidden_dim)

        if target_tokens is not None:
            # Training mode with teacher forcing
            seq_len = target_tokens.shape[1]

            # Embed target tokens
            target_embed = self.embedding(target_tokens)  # (batch, seq_len, embed_dim)

            # Add positional encoding
            positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
            target_embed = target_embed + self.positional_encoding(positions)

            # Apply transformer decoder layers with cross-attention
            hidden = target_embed
            for i, layer in enumerate(self.decoder_layers):
                hidden = layer(
                    hidden,
                    memory=source_embeddings,
                    decoder_state=decoder_states[:, i, :],
                )

            # Output projection
            logits = self.output_projection(hidden)  # (batch, seq_len, vocab_size)

            # Optionally apply copy mechanism
            if source_tokens is not None and source_embeddings is not None:
                # Compute copy attention scores
                copy_scores, _ = self.copy_attention(
                    hidden, source_embeddings, source_embeddings
                )  # (batch, seq_len, src_len)

                # Gate between generation and copying
                gate_input = torch.cat([hidden, copy_scores], dim=-1)
                copy_gate = torch.sigmoid(self.copy_gate(gate_input))  # (batch, seq_len, 1)

                # Mix generation and copy distributions
                # This is simplified - full implementation would expand source vocab
                logits = (1 - copy_gate) * logits + copy_gate * logits.mean(dim=-1, keepdim=True)

            # Compute loss
            loss = F.cross_entropy(
                logits.reshape(-1, self.vocab_size),
                target_tokens.reshape(-1),
                ignore_index=0,  # Padding token
            )

            return logits, loss

        else:
            # Inference mode with beam search or sampling
            return self.beam_search_decode(
                z_semantic,
                source_embeddings=source_embeddings,
                source_tokens=source_tokens,
                beam_width=5,
                max_length=128,
                temperature=temperature,
            ), None

    def beam_search_decode(
        self,
        z_semantic: Tensor,
        source_embeddings: Optional[Tensor] = None,
        source_tokens: Optional[Tensor] = None,
        beam_width: int = 5,
        max_length: int = 128,
        temperature: float = 0.7,
        diversity_penalty: float = 0.5,
    ) -> Tensor:
        """
        Beam search decoding with diversity penalty.

        Returns:
            generated_tokens: (batch, max_length) - Top beam result
        """
        batch_size = z_semantic.shape[0]
        device = z_semantic.device

        # Initialize beams
        beams = [
            {
                'tokens': torch.tensor([[2]], device=device),  # Start with BOS
                'score': 0.0,
                'finished': False,
            }
            for _ in range(batch_size)
        ]

        # Decoder states from latent
        decoder_states = self.latent_projection(z_semantic)
        decoder_states = decoder_states.view(batch_size, self.num_layers, self.hidden_dim)

        # Beam search
        for step in range(max_length):
            all_candidates = []

            for batch_idx in range(batch_size):
                batch_beams = beams[batch_idx * beam_width:(batch_idx + 1) * beam_width] \
                             if step > 0 else [beams[batch_idx]]

                for beam in batch_beams:
                    if beam['finished']:
                        all_candidates.append(beam)
                        continue

                    # Get current tokens
                    current_tokens = beam['tokens']

                    # Embed and add positional encoding
                    token_embed = self.embedding(current_tokens[:, -1:])
                    pos = torch.tensor([[current_tokens.shape[1] - 1]], device=device)
                    token_embed = token_embed + self.positional_encoding(pos)

                    # Apply decoder layers
                    hidden = token_embed
                    for i, layer in enumerate(self.decoder_layers):
                        hidden = layer(
                            hidden,
                            memory=source_embeddings[batch_idx:batch_idx+1] if source_embeddings is not None else None,
                            decoder_state=decoder_states[batch_idx:batch_idx+1, i, :],
                        )

                    # Get logits
                    logits = self.output_projection(hidden).squeeze(1)  # (vocab_size,)

                    # Apply temperature
                    logits = logits / temperature

                    # Apply diversity penalty for repeated n-grams
                    if step > 2:
                        for n in [2, 3]:  # Check bigrams and trigrams
                            if current_tokens.shape[1] >= n:
                                recent_ngram = tuple(current_tokens[0, -n:].tolist())
                                # Count occurrences
                                count = 0
                                for i in range(current_tokens.shape[1] - n):
                                    ngram = tuple(current_tokens[0, i:i+n].tolist())
                                    if ngram == recent_ngram:
                                        count += 1
                                if count > 0:
                                    # Apply penalty
                                    logits = logits - diversity_penalty * count

                    # Get top-k tokens
                    log_probs = F.log_softmax(logits, dim=-1)
                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)

                    # Create new beams
                    for log_prob, token_id in zip(top_log_probs, top_indices):
                        new_tokens = torch.cat([current_tokens, token_id.unsqueeze(0).unsqueeze(0)], dim=1)
                        new_score = beam['score'] + log_prob.item()

                        # Length normalization
                        normalized_score = new_score / (new_tokens.shape[1] ** 0.6)

                        new_beam = {
                            'tokens': new_tokens,
                            'score': new_score,
                            'normalized_score': normalized_score,
                            'finished': token_id.item() == 3,  # EOS token
                        }
                        all_candidates.append(new_beam)

            # Select top beams for next step
            all_candidates.sort(key=lambda x: x['normalized_score'], reverse=True)
            beams = all_candidates[:batch_size * beam_width]

            # Check if all beams are finished
            if all(beam['finished'] for beam in beams[:batch_size]):
                break

        # Select best beam for each batch
        result = []
        for batch_idx in range(batch_size):
            batch_beams = beams[batch_idx * beam_width:(batch_idx + 1) * beam_width]
            best_beam = max(batch_beams, key=lambda x: x['normalized_score'])
            tokens = best_beam['tokens'][0]

            # Pad to max_length
            if tokens.shape[0] < max_length:
                padding = torch.zeros(max_length - tokens.shape[0], dtype=torch.long, device=device)
                tokens = torch.cat([tokens, padding])
            else:
                tokens = tokens[:max_length]

            result.append(tokens)

        return torch.stack(result)


class TransformerDecoderLayer(nn.Module):
    """Single transformer decoder layer with cross-attention."""

    def __init__(
        self,
        d_model: int,
        d_hidden: int,
        n_heads: int,
        dropout: float = 0.1,
    ):
        super().__init__()

        # Self-attention
        self.self_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True,
        )
        self.norm1 = nn.LayerNorm(d_model)

        # Cross-attention (optional - only if memory provided)
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True,
        )
        self.norm2 = nn.LayerNorm(d_model)

        # Feed-forward
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_hidden, d_model),
        )
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        x: Tensor,
        memory: Optional[Tensor] = None,
        decoder_state: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor (batch, seq_len, d_model)
            memory: Encoder output for cross-attention (batch, src_len, d_model)
            decoder_state: Additional decoder state (batch, d_hidden)
        """
        # Self-attention with causal mask
        seq_len = x.shape[1]
        device = x.device
        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device) * float('-inf'), diagonal=1)

        attn_out, _ = self.self_attn(x, x, x, attn_mask=causal_mask)
        x = self.norm1(x + self.dropout(attn_out))

        # Cross-attention (if encoder output provided)
        if memory is not None:
            cross_out, _ = self.cross_attn(x, memory, memory)
            x = self.norm2(x + self.dropout(cross_out))

        # Feed-forward
        ffn_out = self.ffn(x)
        x = self.norm3(x + self.dropout(ffn_out))

        return x


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  QUALITY FILTERING - Filter training chunks for high-quality docstrings
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def filter_quality_chunks(chunks: List['CodeChunk'], min_length: int = 50) -> List['CodeChunk']:
    """
    Filter chunks to keep only high-quality documentation.

    Criteria:
    1. Explanation length > min_length characters
    2. Not auto-generated (TODO, FIXME, etc.)
    3. Informativeness score > 0.7 (unique words / total words)
    4. Contains actual description (not just function signature)
    """
    filtered = []

    # Patterns for low-quality docstrings
    LOW_QUALITY_PATTERNS = [
        r'^(TODO|FIXME|XXX|HACK|NOTE|WARNING):?\s*$',
        r'^(todo|fixme|xxx|hack|note|warning):?\s*$',
        r'^\s*$',  # Empty or whitespace only
        r'^[A-Za-z_][A-Za-z0-9_]*$',  # Single word (likely just function name)
        r'^(Not implemented|Placeholder|TBD|None)\.?$',
        r'^\W+$',  # Only punctuation
    ]

    for chunk in chunks:
        if not chunk.explanation_target:
            continue

        explanation = chunk.explanation_target.strip()

        # Check length
        if len(explanation) < min_length:
            continue

        # Check for auto-generated patterns
        is_low_quality = False
        for pattern in LOW_QUALITY_PATTERNS:
            if re.match(pattern, explanation):
                is_low_quality = True
                break

        if is_low_quality:
            continue

        # Check informativeness (unique words / total words)
        words = explanation.lower().split()
        if len(words) > 0:
            unique_words = len(set(words))
            informativeness = unique_words / len(words)
            if informativeness < 0.7:
                continue
        else:
            continue

        # Check if it contains actual description (more than just code elements)
        # Simple heuristic: should contain some common English words
        common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                       'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                       'should', 'could', 'may', 'might', 'must', 'shall', 'can',
                       'this', 'that', 'these', 'those', 'with', 'for', 'to', 'from',
                       'of', 'in', 'on', 'at', 'by', 'and', 'or', 'but', 'if', 'then'}

        words_lower = set(w.lower() for w in words)
        if not words_lower.intersection(common_words):
            continue

        # Passed all filters
        filtered.append(chunk)

    return filtered


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  IMPROVED NEURAL CODE SEARCH MODEL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedNeuralCodeSearchModel(nn.Module):
    """
    Complete neural code search model with high-quality explanation generation.

    Key improvements:
    1. BPE tokenization for both code and explanations
    2. Cross-attention in decoder
    3. Beam search decoding
    4. Quality-filtered training
    """

    def __init__(
        self,
        bpe_tokenizer: BPETokenizer,
        embed_dim: int = 256,
        hidden_dim: int = 512,
        latent_dim: int = 32,
    ):
        super().__init__()
        self.bpe_tokenizer = bpe_tokenizer
        self.vocab_size = bpe_tokenizer.vocab_size
        self.latent_dim = latent_dim

        # Code encoder (for source tokens)
        self.code_embedding = nn.Embedding(self.vocab_size, embed_dim, padding_idx=0)
        self.code_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=8,
                dim_feedforward=hidden_dim,
                dropout=0.1,
                batch_first=True,
            ),
            num_layers=4,
        )

        # Latent encoder (code -> latent distribution)
        self.latent_encoder = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.enc_mean = nn.Linear(hidden_dim, latent_dim)
        self.enc_logvar = nn.Linear(hidden_dim, latent_dim)

        # SDE dynamics
        self.dynamics = RaccoonDynamics(latent_dim, hidden_dim, sigma_min=1e-4, sigma_max=1.0)

        # Normalizing flow
        self.flow = RaccoonFlow(latent_dim, hidden_dim, num_layers=4, time_dim=32)

        # Improved explanation decoder
        self.decoder = ImprovedExplanationDecoder(
            vocab_size=self.vocab_size,
            embed_dim=embed_dim,
            latent_dim=latent_dim,
            hidden_dim=hidden_dim,
            num_layers=3,
            num_heads=8,
            dropout=0.1,
        )

        # Latent regularization
        univariate_test = FastEppsPulley(t_max=5.0, n_points=17)
        self.latent_test = SlicingUnivariateTest(
            univariate_test=univariate_test,
            num_slices=64,
            reduction="mean",
        )

    def encode(self, tokens: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Encode tokens to latent distribution and source embeddings.

        Returns:
            mean: (batch, latent_dim)
            logvar: (batch, latent_dim)
            source_embeddings: (batch, seq_len, embed_dim) - for cross-attention
        """
        # Embed tokens
        x = self.code_embedding(tokens)  # (batch, seq_len, embed_dim)

        # Encode with transformer
        mask = (tokens == 0)  # Padding mask
        source_embeddings = self.code_encoder(x, src_key_padding_mask=mask)

        # Pool for latent encoding (mean pooling over non-padded tokens)
        mask_expanded = mask.unsqueeze(-1).expand_as(source_embeddings)
        pooled = (source_embeddings * ~mask_expanded).sum(dim=1) / (~mask_expanded).sum(dim=1)

        # Encode to latent
        h = self.latent_encoder(pooled)
        mean = self.enc_mean(h)
        logvar = self.enc_logvar(h)

        return mean, logvar, source_embeddings

    def reparameterize(self, mean: Tensor, logvar: Tensor) -> Tensor:
        """Reparameterization trick."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mean + eps * std

    def encode_to_semantic(self, tokens: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Full encoding pipeline to semantic space.

        Returns:
            z_semantic: (batch, latent_dim)
            source_embeddings: (batch, seq_len, embed_dim)
        """
        # Encode
        mean, logvar, source_embeddings = self.encode(tokens)
        z0 = self.reparameterize(mean, logvar)

        # SDE trajectory
        device = tokens.device
        t_span = torch.linspace(0.0, 0.1, 3, device=device)
        z_traj = solve_sde(self.dynamics, z0, t_span)
        z_endpoint = z_traj[:, -1, :]

        # Normalizing flow
        t_flow = torch.ones(z0.shape[0], 1, device=device) * 0.5
        z_semantic, _ = self.flow(z_endpoint, t_flow, reverse=False)

        return z_semantic, source_embeddings

    def generate_explanation(
        self,
        z_semantic: Tensor,
        source_tokens: Optional[Tensor] = None,
        source_embeddings: Optional[Tensor] = None,
        temperature: float = 0.7,
        max_len: int = 128,
    ) -> str:
        """
        Generate human-readable explanation.

        Returns:
            explanation: String explanation
        """
        # Generate tokens using improved decoder
        generated_tokens, _ = self.decoder(
            z_semantic,
            source_tokens=source_tokens,
            source_embeddings=source_embeddings,
            target_tokens=None,
            temperature=temperature,
        )

        # Decode to text
        explanation = self.bpe_tokenizer.decode(generated_tokens[0].cpu().tolist())
        return explanation

    def forward(
        self,
        code_tokens: Tensor,
        explanation_tokens: Optional[Tensor] = None,
        loss_weights: Tuple[float, float, float] = (1.0, 0.1, 0.01),
    ) -> Tuple[Tensor, Dict[str, Tensor]]:
        """
        Forward pass with loss computation.
        """
        batch_size = code_tokens.shape[0]
        device = code_tokens.device

        # Encode
        mean, logvar, source_embeddings = self.encode(code_tokens)
        z0 = self.reparameterize(mean, logvar)

        # SDE trajectory
        t_span = torch.linspace(0.0, 0.1, 3, device=device)
        z_traj = solve_sde(self.dynamics, z0, t_span)
        z_endpoint = z_traj[:, -1, :]

        # Normalizing flow
        t_flow = torch.ones(batch_size, 1, device=device) * 0.5
        z_semantic, log_det = self.flow(z_endpoint, t_flow, reverse=False)

        # Explanation loss
        explanation_loss = torch.tensor(0.0, device=device)
        if explanation_tokens is not None:
            _, exp_loss = self.decoder(
                z_semantic,
                source_tokens=code_tokens,
                source_embeddings=source_embeddings,
                target_tokens=explanation_tokens,
            )
            explanation_loss = exp_loss

        # KL divergence
        kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())
        kl_loss = torch.clamp(kl_loss, min=0.0)

        # Epps-Pulley regularization
        z_for_test = z_semantic.unsqueeze(0)
        ep_loss = self.latent_test(z_for_test)

        # Total loss
        w_exp, w_kl, w_ep = loss_weights
        loss = w_exp * explanation_loss + w_kl * kl_loss + w_ep * ep_loss

        stats = {
            'explanation_loss': explanation_loss.detach(),
            'kl_loss': kl_loss.detach(),
            'ep_loss': ep_loss.detach(),
            'z_semantic': z_semantic.detach(),
        }

        return loss, stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MAIN EXECUTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("ğŸš€ Improved Neural Code Search with BPE and Beam Search")
    print("="*80)

    # Initialize BPE tokenizer
    tokenizer = BPETokenizer()

    # Test encoding/decoding
    test_text = "This function implements SDE dynamics with drift and diffusion networks"
    tokens = tokenizer.encode(test_text, max_length=128)
    decoded = tokenizer.decode(tokens)
    print(f"\nğŸ“ BPE Test:")
    print(f"  Original: {test_text}")
    print(f"  Tokens: {tokens[:20]}... (length: {len(tokens)})")
    print(f"  Decoded: {decoded}")

    # Initialize improved model
    model = ImprovedNeuralCodeSearchModel(
        bpe_tokenizer=tokenizer,
        embed_dim=256,
        hidden_dim=512,
        latent_dim=32,
    )

    param_count = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ¤– Model initialized:")
    print(f"  Parameters: {param_count:,}")
    print(f"  Vocab size: {tokenizer.vocab_size:,}")
    print(f"  Decoder layers: 3")
    print(f"  Attention heads: 8")

    # Test forward pass
    print(f"\nğŸ”¬ Testing forward pass...")
    batch_size = 2

    # Sample code and explanations
    sample_codes = [
        "def solve_sde(dynamics, z0, t_span): return integrate(dynamics, z0, t_span)",
        "class RaccoonFlow(nn.Module): def __init__(self): super().__init__()"
    ]
    sample_explanations = [
        "Solves stochastic differential equations using numerical integration",
        "Normalizing flow model for invertible transformations"
    ]

    # Encode with BPE
    code_tokens = tokenizer.batch_encode(sample_codes, max_length=128)
    explanation_tokens = tokenizer.batch_encode(sample_explanations, max_length=128)

    # Forward pass
    loss, stats = model(code_tokens, explanation_tokens)
    print(f"  Loss: {loss.item():.4f}")
    print(f"  Explanation loss: {stats['explanation_loss'].item():.4f}")
    print(f"  KL loss: {stats['kl_loss'].item():.4f}")

    # Test generation
    print(f"\nâœ¨ Testing explanation generation...")
    model.eval()
    with torch.no_grad():
        z_semantic, source_embeddings = model.encode_to_semantic(code_tokens[:1])
        explanation = model.generate_explanation(
            z_semantic,
            source_tokens=code_tokens[:1],
            source_embeddings=source_embeddings,
            temperature=0.7,
        )
        print(f"  Generated: {explanation}")

    print(f"\nâœ… All tests passed! Model ready for training.")
    print(f"\nğŸ“‹ Key Improvements:")
    print(f"  1. BPE tokenization with {tokenizer.vocab_size:,} vocab")
    print(f"  2. Cross-attention over source code")
    print(f"  3. Beam search with diversity penalty")
    print(f"  4. Quality filtering for training")
    print(f"  5. Temperature-controlled sampling")
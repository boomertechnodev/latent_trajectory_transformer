"""
Integration Guide: Enhanced Normalizing Flows into Raccoon Architecture
========================================================================

This file shows how to integrate the enhanced flow components into
the existing latent_drift_trajectory.py file.

Author: Normalizing Flows Specialist Agent
Date: 2025-11-16
"""

import torch
import torch.nn as nn
from typing import Tuple
from torch import Tensor

# Import enhanced components
from improved_normalizing_flows import (
    EnhancedRaccoonFlow,
    ActNorm,
    Invertible1x1Conv,
    NeuralSplineCoupling,
    ImprovedAffineCoupling,
    test_normalizing_flow
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MODIFIED RACCOON LOG CLASSIFIER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EnhancedRaccoonLogClassifier(nn.Module):
    """
    Drop-in replacement for RaccoonLogClassifier (lines 1353-1582)
    with enhanced normalizing flow components.

    Key improvements:
    1. Uses EnhancedRaccoonFlow with ActNorm + Spline/Affine + 1x1 Conv
    2. Monitors log-det statistics for stability
    3. Includes invertibility verification
    4. Better numerical stability throughout
    """

    def __init__(
        self,
        vocab_size: int,
        num_classes: int,
        latent_dim: int = 32,
        hidden_dim: int = 64,
        embed_dim: int = 32,
        memory_size: int = 5000,
        adaptation_rate: float = 1e-4,
        # NEW: Flow configuration
        flow_layers: int = 8,
        flow_type: str = 'spline',  # 'spline' or 'affine'
        use_actnorm: bool = True,
        use_1x1_conv: bool = True,
    ):
        super().__init__()
        self.latent_dim = latent_dim
        self.num_classes = num_classes
        self.adaptation_rate = adaptation_rate

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ENCODER (unchanged from original)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.encoder = nn.Sequential(
            nn.Embedding(vocab_size, embed_dim),
            nn.Linear(embed_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
        )
        self.enc_mean = nn.Linear(hidden_dim, latent_dim)
        self.enc_logvar = nn.Linear(hidden_dim, latent_dim)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # SDE DYNAMICS (unchanged)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Import RaccoonDynamics from original file
        from latent_drift_trajectory import RaccoonDynamics
        self.dynamics = RaccoonDynamics(
            latent_dim, hidden_dim,
            sigma_min=1e-4, sigma_max=1.0
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ENHANCED NORMALIZING FLOW (NEW!)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.flow = EnhancedRaccoonFlow(
            latent_dim=latent_dim,
            hidden_dim=hidden_dim,
            num_layers=flow_layers,
            coupling_type=flow_type,
            use_1x1_conv=use_1x1_conv,
            use_actnorm=use_actnorm
        )

        # Track flow health metrics
        self.register_buffer('flow_invertibility_score', torch.ones(1))
        self.register_buffer('flow_check_counter', torch.zeros(1))

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # CLASSIFIER & REGULARIZATION (unchanged)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, num_classes)
        )

        # Import EP test from original
        from latent_drift_trajectory import FastEppsPulley, SlicingUnivariateTest
        univariate_test = FastEppsPulley(t_max=5.0, n_points=17)
        self.latent_test = SlicingUnivariateTest(
            univariate_test=univariate_test,
            num_slices=64,
            reduction="mean",
        )

        # Experience replay
        from latent_drift_trajectory import RaccoonMemory
        self.memory = RaccoonMemory(max_size=memory_size)

        # Learnable prior
        self.z0_mean = nn.Parameter(torch.zeros(latent_dim))
        self.z0_logvar = nn.Parameter(torch.zeros(latent_dim))

    @torch.no_grad()
    def check_flow_health(self, z: Tensor, t: Tensor):
        """
        Periodically verify flow invertibility and numerical stability.
        Called every 100 forward passes during training.
        """
        self.flow_check_counter += 1

        if self.flow_check_counter % 100 == 0:
            metrics = self.flow.check_invertibility(z[:8], t[:8])

            # Update health score (exponential moving average)
            alpha = 0.1
            health = float(metrics['is_invertible'])
            self.flow_invertibility_score = (
                (1 - alpha) * self.flow_invertibility_score +
                alpha * health
            )

            # Warning if health degrades
            if self.flow_invertibility_score < 0.9:
                print(f"âš ï¸  Flow Health Warning: score={self.flow_invertibility_score:.2f}")
                print(f"   Max error: {metrics['max_error']:.6f}")
                print(f"   Log-det mean: {metrics['log_det_forward_mean']:.2f}")

    def encode(self, tokens: Tensor) -> Tuple[Tensor, Tensor]:
        """Encode tokens to latent distribution (unchanged)."""
        x = self.encoder(tokens)  # (batch, seq_len, hidden)
        x = x.mean(dim=1)  # (batch, hidden)

        mean = self.enc_mean(x)
        logvar = self.enc_logvar(x)

        return mean, logvar

    def sample_latent(self, mean: Tensor, logvar: Tensor) -> Tensor:
        """Reparameterization trick (unchanged)."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(
        self,
        tokens: Tensor,
        labels: Tensor,
        loss_weights: Tuple[float, float, float] = (1.0, 0.1, 0.01),
    ):
        """
        Forward pass with enhanced flow and stability monitoring.
        """
        batch_size = tokens.shape[0]

        # Guard against empty batches
        if batch_size == 0:
            return torch.tensor(0.0, device=tokens.device), {
                "class_loss": torch.tensor(0.0),
                "kl_loss": torch.tensor(0.0),
                "ep_loss": torch.tensor(0.0),
                "accuracy": torch.tensor(0.0),
            }

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ENCODE â†’ SDE â†’ FLOW (with enhanced components)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        mean, logvar = self.encode(tokens)
        z = self.sample_latent(mean, logvar)

        # Apply SDE dynamics
        from latent_drift_trajectory import solve_sde
        t_span = torch.linspace(0.0, 0.1, 3, device=z.device)
        z_traj = solve_sde(self.dynamics, z, t_span)
        z = z_traj[:, -1, :]

        # Apply ENHANCED normalizing flow
        t = torch.ones(batch_size, 1, device=z.device) * 0.5
        z_flow, log_det = self.flow(z, t, reverse=False)

        # Monitor flow health during training
        if self.training:
            self.check_flow_health(z, t)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # LOSSES (with improved numerical stability)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # Classification loss
        logits = self.classifier(z_flow)
        class_loss = nn.functional.cross_entropy(logits, labels)

        # KL divergence (with numerical stability improvements)
        var_q = torch.exp(logvar)
        var_p = torch.exp(self.z0_logvar)

        # Add small epsilon for stability
        eps = 1e-8
        kl_loss = 0.5 * torch.mean(
            self.z0_logvar - logvar +
            (var_q + (mean - self.z0_mean).pow(2)) / (var_p + eps) -
            1
        )

        # Include flow log-det in KL (proper ELBO)
        # This accounts for the change of variables from flow
        kl_loss = kl_loss - log_det.mean() / self.latent_dim

        # Clamp to ensure non-negative (with gradient preservation)
        kl_loss = torch.maximum(kl_loss, torch.tensor(0.0, device=kl_loss.device))

        # Epps-Pulley regularization
        z_for_test = z_flow.unsqueeze(0)  # (1, batch, latent)
        ep_loss = self.latent_test(z_for_test)

        # Total loss
        w_class, w_kl, w_ep = loss_weights
        loss = w_class * class_loss + w_kl * kl_loss + w_ep * ep_loss

        # Accuracy
        with torch.no_grad():
            preds = logits.argmax(dim=1)
            acc = (preds == labels).float().mean()

        stats = {
            "class_loss": class_loss.detach(),
            "kl_loss": kl_loss.detach(),
            "ep_loss": ep_loss.detach(),
            "accuracy": acc.detach(),
            "log_det": log_det.mean().detach(),  # NEW: track log-det
            "flow_health": self.flow_invertibility_score.item(),  # NEW: track health
        }

        return loss, stats

    def continuous_update(self, tokens: Tensor, labels: Tensor):
        """
        Online update with memory replay (unchanged from original).
        """
        # [Implementation unchanged - same as original RaccoonLogClassifier]
        # Just using the enhanced flow internally via forward()
        pass  # See original implementation lines 1523-1582


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  INTEGRATION INSTRUCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def integrate_enhanced_flows():
    """
    Step-by-step instructions for integrating enhanced flows.

    OPTION 1: Drop-in Replacement (Recommended)
    ============================================
    1. Import the enhanced components at the top of latent_drift_trajectory.py:

       from improved_normalizing_flows import (
           EnhancedRaccoonFlow,
           test_normalizing_flow
       )

    2. Replace RaccoonFlow (lines 1129-1180) with EnhancedRaccoonFlow

    3. In RaccoonLogClassifier.__init__ (around line 1387), replace:

       OLD:
       self.flow = RaccoonFlow(latent_dim, hidden_dim, num_layers=4)

       NEW:
       self.flow = EnhancedRaccoonFlow(
           latent_dim, hidden_dim,
           num_layers=8,
           coupling_type='spline',
           use_1x1_conv=True,
           use_actnorm=True
       )

    4. Add flow health monitoring in forward() (around line 1480):

       # After applying flow
       if self.training and hasattr(self.flow, 'check_invertibility'):
           if random.random() < 0.01:  # Check 1% of batches
               metrics = self.flow.check_invertibility(z[:8], t[:8])
               if not metrics['is_invertible']:
                   print(f"âš ï¸ Flow invertibility issue: {metrics}")


    OPTION 2: Gradual Integration (Conservative)
    =============================================
    1. Keep original components but add ActNorm layers:

       # In RaccoonFlow.__init__, add before each coupling layer:
       self.actnorms = nn.ModuleList([
           ActNorm(latent_dim) for _ in range(num_layers)
       ])

       # In RaccoonFlow.forward, apply ActNorm before coupling:
       for i, (actnorm, flow) in enumerate(zip(self.actnorms, self.flows)):
           z, log_det_act = actnorm(z, reverse=reverse)
           log_det_sum += log_det_act
           z, log_det_flow = flow(z, time_features, reverse=reverse)
           log_det_sum += log_det_flow

    2. Add 1x1 convolutions between coupling layers:

       # In RaccoonFlow.__init__:
       self.convs = nn.ModuleList([
           Invertible1x1Conv(latent_dim, 'lu')
           for _ in range(num_layers // 2)
       ])

    3. Upgrade to neural spline coupling:

       # Replace CouplingLayer with NeuralSplineCoupling
       # This requires more changes but provides better expressiveness


    OPTION 3: A/B Testing Setup
    ============================
    Create both models and compare:

    # Original model
    model_original = RaccoonLogClassifier(...)

    # Enhanced model
    model_enhanced = EnhancedRaccoonLogClassifier(...)

    # Train both and compare metrics
    """
    pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TESTING THE INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_integration():
    """
    Test that enhanced components work correctly.
    """
    print("ğŸ§ª Testing Enhanced Raccoon Integration")
    print("="*60)

    # Configuration
    device = torch.device('cpu')
    vocab_size = 39  # From log_vocab_size
    num_classes = 4
    batch_size = 8

    # Create enhanced model
    print("\nğŸ“¦ Creating Enhanced Model...")
    model = EnhancedRaccoonLogClassifier(
        vocab_size=vocab_size,
        num_classes=num_classes,
        latent_dim=32,
        hidden_dim=64,
        embed_dim=32,
        flow_layers=8,
        flow_type='spline',
        use_actnorm=True,
        use_1x1_conv=True
    ).to(device)

    param_count = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {param_count:,}")
    flow_params = sum(p.numel() for p in model.flow.parameters())
    print(f"   Flow parameters: {flow_params:,}")

    # Test forward pass
    print("\nğŸ”„ Testing Forward Pass...")
    tokens = torch.randint(0, vocab_size, (batch_size, 50), device=device)
    labels = torch.randint(0, num_classes, (batch_size,), device=device)

    loss, stats = model(tokens, labels)
    print(f"   Loss: {loss.item():.4f}")
    for key, value in stats.items():
        if isinstance(value, torch.Tensor):
            print(f"   {key}: {value.item():.4f}")
        else:
            print(f"   {key}: {value:.4f}")

    # Test flow specifically
    print("\nğŸŒŠ Testing Flow Component...")
    test_normalizing_flow(model.flow, latent_dim=32, batch_size=8, device='cpu')

    print("\nâœ… Integration test complete!")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PERFORMANCE COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compare_flows():
    """
    Compare original vs enhanced flow performance.
    """
    print("ğŸ“Š Flow Performance Comparison")
    print("="*60)

    import time
    import torch

    latent_dim = 32
    hidden_dim = 64
    batch_size = 64
    device = 'cpu'

    # Original flow (simplified affine coupling)
    from latent_drift_trajectory import RaccoonFlow
    original_flow = RaccoonFlow(latent_dim, hidden_dim, num_layers=4)

    # Enhanced flow
    enhanced_flow = EnhancedRaccoonFlow(
        latent_dim, hidden_dim,
        num_layers=8,
        coupling_type='affine',  # Fair comparison
        use_1x1_conv=True,
        use_actnorm=True
    )

    # Test data
    z = torch.randn(batch_size, latent_dim)
    t = torch.ones(batch_size, 1) * 0.5

    print("\nğŸ“ Model Sizes:")
    print(f"   Original: {sum(p.numel() for p in original_flow.parameters()):,} params")
    print(f"   Enhanced: {sum(p.numel() for p in enhanced_flow.parameters()):,} params")

    print("\nâ±ï¸  Speed Test (100 forward passes):")
    for name, flow in [("Original", original_flow), ("Enhanced", enhanced_flow)]:
        flow.eval()
        start = time.time()

        for _ in range(100):
            with torch.no_grad():
                z_out, log_det = flow(z, t, reverse=False)

        elapsed = time.time() - start
        print(f"   {name}: {elapsed:.3f}s ({elapsed/100*1000:.1f}ms per batch)")

    print("\nğŸ¯ Expressiveness Test:")
    # Test how well each flow can transform Gaussian to complex distribution
    z_test = torch.randn(1000, latent_dim)
    t_test = torch.ones(1000, 1) * 0.5

    with torch.no_grad():
        # Original
        z_orig, log_det_orig = original_flow(z_test, t_test, reverse=False)
        var_orig = z_orig.var(dim=0).mean().item()
        log_det_mean_orig = log_det_orig.mean().item()

        # Enhanced
        z_enh, log_det_enh = enhanced_flow(z_test, t_test, reverse=False)
        var_enh = z_enh.var(dim=0).mean().item()
        log_det_mean_enh = log_det_enh.mean().item()

    print(f"   Original - Variance: {var_orig:.3f}, Log-det: {log_det_mean_orig:.3f}")
    print(f"   Enhanced - Variance: {var_enh:.3f}, Log-det: {log_det_mean_enh:.3f}")

    print("\nâœ… Comparison complete!")


if __name__ == "__main__":
    print("ğŸ”¬ NORMALIZING FLOW INTEGRATION GUIDE")
    print("="*80)
    print("\nThis guide shows how to integrate enhanced flows into Raccoon.")
    print("\nRunning integration tests...\n")

    # Run tests
    test_integration()
    print("\n" + "-"*80 + "\n")
    compare_flows()